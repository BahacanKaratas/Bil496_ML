{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfiguredMetric:\n",
    "    def __init__(self, metric, *metric_args, **metric_kwargs):\n",
    "        self.metric = metric\n",
    "        self.metric_args = metric_args\n",
    "        self.metric_kwargs = metric_kwargs\n",
    "    \n",
    "    def add(self, *args, **kwargs):\n",
    "        return self.metric.add(*args, **kwargs)\n",
    "    \n",
    "    def add_batch(self, *args, **kwargs):\n",
    "        return self.metric.add_batch(*args, **kwargs)\n",
    "\n",
    "    def compute(self, *args, **kwargs):\n",
    "        return self.metric.compute(*args, *self.metric_args, **kwargs, **self.metric_kwargs)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.metric.name\n",
    "\n",
    "    def _feature_names(self):\n",
    "        return self.metric._feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nimport pandas as pd\\nimport mysql.connector\\n\\n# Database connection parameters\\nhost = \"your_host\"\\nport = 3306  # default MySQL port\\nuser = \"your_username\"\\npassword = \"your_password\"\\ndatabase = \"your_database_name\"\\n\\n# Establishing the connection\\nconn = mysql.connector.connect(host=host, port=port, user=user, password=password, database=database)\\n\\n# SQL query\\nquery = \"SELECT column1, column2, column3 FROM your_table_name\"\\n\\n# Fetching data into DataFrame\\ndf = pd.read_sql(query, conn)\\n\\n# Renaming columns if necessary\\ndf.rename(columns={\\'Participants\\': \\'label\\'}, inplace=True)\\n\\n# Display the first few rows of the DataFrame\\nprint(df.head())\\n\\n# Close the connection\\nconn.close()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df=pd.read_csv(\"datasetprime.csv\")\n",
    "df.rename(columns = {'UserID':'label',}, inplace = True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\"\"\"\"\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Database connection parameters\n",
    "host = \"your_host\"\n",
    "port = 3306  # default MySQL port\n",
    "user = \"your_username\"\n",
    "password = \"your_password\"\n",
    "database = \"your_database_name\"\n",
    "\n",
    "# Establishing the connection\n",
    "conn = mysql.connector.connect(host=host, port=port, user=user, password=password, database=database)\n",
    "\n",
    "# SQL query\n",
    "query = \"SELECT column1, column2, column3 FROM your_table_name\"\n",
    "\n",
    "# Fetching data into DataFrame\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# Renaming columns if necessary\n",
    "df.rename(columns={'Participants': 'label'}, inplace=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns_to_text(df, exclude_columns=None, column_weights=None):\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "    if column_weights is None:\n",
    "        column_weights = {}\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=['text', 'label'])\n",
    "    \n",
    "    # Store the 'label' column if it exists and is not in the exclude list\n",
    "    label_col = df['label'] if 'label' in df.columns and 'label' not in exclude_columns else None\n",
    "    \n",
    "    # Drop the columns that need to be excluded\n",
    "    df_combined = df.drop(columns=exclude_columns, errors='ignore')\n",
    "    \n",
    "    # Apply weighting to specified columns by repeating their contents along with the column name\n",
    "    for column, weight in column_weights.items():\n",
    "        if column in df_combined.columns:\n",
    "            df_combined[column] = df_combined[column].apply(\n",
    "                lambda x: (' '.join([f\"{column}:{x}\"] * weight)) if pd.notnull(x) else ''\n",
    "            )\n",
    "    \n",
    "    # Combine all columns into a single column\n",
    "    df_combined['text'] = df_combined.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    \n",
    "    # Include the 'label' column in the result if it was stored\n",
    "    if label_col is not None:\n",
    "        df_combined['label'] = label_col\n",
    "    \n",
    "    # Return the DataFrame with the 'text' and 'label' columns\n",
    "    return df_combined[['text', 'label']] if label_col is not None else df_combined[['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for each column\n",
    "column_weights = {\n",
    "    'Username': 0,\n",
    "    'Email': 0,\n",
    "    'Gender': 0,\n",
    "    'Age': 0,\n",
    "    'Heig   ht': 0,\n",
    "    'Weight': 0,\n",
    "    'ActivityLevel': 15,\n",
    "    'GoalID': 10,\n",
    "    'DietPreferenceID': 15,\n",
    "    'RegionID': 15,\n",
    "    'AllergenID': 30  # Higher weight for AllergenID\n",
    "}\n",
    "\n",
    "# Combine columns with specified weights\n",
    "generalDataframe = combine_columns_to_text(df, exclude_columns=['DietID'], column_weights=column_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 False       ActivityLevel:Sedentary Activity...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 True       ActivityLevel:Very Active Activit...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 True       ActivityLevel:Active ActivityLeve...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 False       ActivityLevel:Sedentary Activity...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 False       ActivityLevel:Active ActivityLev...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  1 False       ActivityLevel:Sedentary Activity...      1\n",
       "1  2 True       ActivityLevel:Very Active Activit...      2\n",
       "2  3 True       ActivityLevel:Active ActivityLeve...      3\n",
       "3  4 False       ActivityLevel:Sedentary Activity...      4\n",
       "4  5 False       ActivityLevel:Active ActivityLev...      5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalDataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "generalDataframe['label'] = label_encoder.fit_transform(generalDataframe['label'])\n",
    "\n",
    "# Split dataset\n",
    "train_df, val_df = train_test_split(generalDataframe, test_size=0.001, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfClasses=max(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataset=Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=numberOfClasses+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/9366 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 3/9366 [00:32<27:32:44, 10.59s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m\n\u001b[0;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     12\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m     23\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(tokenized_val_dataset)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1644\u001b[0m )\n\u001b[1;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1938\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1941\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1942\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1943\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1944\u001b[0m ):\n\u001b[0;32m   1945\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1946\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2770\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2768\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2770\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1853\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluation\n",
    "eval_results = trainer.evaluate(tokenized_val_dataset)\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def predict_label(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "    return label_encoder.inverse_transform([predicted_label])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the input text is: 668\n"
     ]
    }
   ],
   "source": [
    "# Example user information with column names\n",
    "text_to_predict = \"\"\"\n",
    "    Username:JohnDoe Email:johndoe@example.com Gender:Male Age:90 Height:180 Weight:80 \n",
    "    ActivityLevel:Sedentary GoalID:10 DietPreferenceID:2 RegionID:8 AllergenID:10\n",
    "    \"\"\"\n",
    "\n",
    "# Predict the label for the input text\n",
    "predicted_label = predict_label(text_to_predict)\n",
    "\n",
    "# Print the predicted label\n",
    "print(f\"The predicted label for the input text is: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(generalDataframe['text'], generalDataframe['label'], random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', strip_accents='unicode', lowercase=True)\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertForSequenceClassification' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m modelNb\u001b[38;5;241m=\u001b[39mMultinomialNB()\n\u001b[0;32m      2\u001b[0m modelNb\u001b[38;5;241m.\u001b[39mfit(X_train_transformed,X_train)\n\u001b[1;32m----> 3\u001b[0m predictions\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(X_test_transformed)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DistilBertForSequenceClassification' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "modelNb=MultinomialNB()\n",
    "modelNb.fit(X_train_transformed,X_train)\n",
    "predictions=modelNb.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train_transformed, X_train)\n",
    "svm_predictions = svm_model_linear.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 12).fit(X_train_transformed, X_train)\n",
    "knn_predictions = knn.predict(X_test_transformed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_label_id_multinomial_nb(text):\n",
    "    text_transformed = vectorizer.transform([text])\n",
    "    return modelNb.predict(text_transformed)[0]\n",
    "\n",
    "def predict_label_id_svm(text):\n",
    "    text_transformed = vectorizer.transform([text])\n",
    "    return svm_model_linear.predict(text_transformed)[0]\n",
    "\n",
    "def predict_label_id_knn(text):\n",
    "    text_transformed = vectorizer.transform([text])\n",
    "    return knn.predict(text_transformed)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes predicted label ID: 465 True       ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary GoalID:10 GoalID:10 GoalID:10 GoalID:10 GoalID:10 GoalID:10 GoalID:10 GoalID:10 GoalID:10 GoalID:10 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 DietPreferenceID:2 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 RegionID:9 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10\n",
      "SVM predicted label ID: 213 False       ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary GoalID:4 GoalID:4 GoalID:4 GoalID:4 GoalID:4 GoalID:4 GoalID:4 GoalID:4 GoalID:4 GoalID:4 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 DietPreferenceID:8 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 RegionID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10\n",
      "KNN predicted label ID: 18 True       ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary ActivityLevel:Sedentary GoalID:7 GoalID:7 GoalID:7 GoalID:7 GoalID:7 GoalID:7 GoalID:7 GoalID:7 GoalID:7 GoalID:7 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 DietPreferenceID:6 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 RegionID:1 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10 AllergenID:10\n"
     ]
    }
   ],
   "source": [
    "text_to_predict = \"\"\"\n",
    "    Username:JohnDoe Email:johndoe@example.com Gender:Male Age:90 Height:180 Weight:80 \n",
    "    ActivityLevel:Sedentary GoalID:10 DietPreferenceID:2 RegionID:10 AllergenID:10\n",
    "    \"\"\"\n",
    "\n",
    "# Predict using Logistic Regression\n",
    "\n",
    "# Predict using Multinomial Naive Bayes\n",
    "mnb_label_id = predict_label_id_multinomial_nb(text_to_predict)\n",
    "print(f\"Multinomial Naive Bayes predicted label ID: {mnb_label_id}\")\n",
    "\n",
    "# Predict using SVM\n",
    "svm_label_id = predict_label_id_svm(text_to_predict)\n",
    "print(f\"SVM predicted label ID: {svm_label_id}\")\n",
    "\n",
    "# Predict using KNN\n",
    "knn_label_id = predict_label_id_knn(text_to_predict)\n",
    "print(f\"KNN predicted label ID: {knn_label_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
