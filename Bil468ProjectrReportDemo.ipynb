{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Bil468 Project Report: Face Recognition Program"]},{"cell_type":"markdown","metadata":{},"source":["### Authors:   \n","- Görkem Ecer\n","- Bahacan Karataş 191101069"]},{"cell_type":"markdown","metadata":{},"source":["# **Chapter 1: Introduction**\n","- Face recognition, a biometric technology, identifies individuals by analyzing their facial features.\n","- Face recognition is a technology that lets computers recognize people by their faces. It's used in things like unlocking your phone with a glance or tagging friends in photos. It's handy and makes things more secure.\n","\n","## **Applications**:\n","\n","### 1. **Security**\n","   - Enhanced security for restricted areas.\n","   - Attendance tracking in organizations.\n","\n","### 2. **User Authentication**\n","   - Unlock smartphones and authorize transactions.\n","   - National ID systems for secure identification.\n","\n","### 3. **Convenience**\n","   - Social media auto-tagging.\n","   - Personalized experiences and advertising.\n","\n","### 4. **Healthcare**\n","   - Accurate patient identification.\n","   - Detection of medical conditions through facial analysis.\n","\n","## Advantages:\n","- Non-intrusive and user-friendly.\n","- Versatile applications.\n","- High accuracy when properly implemented.\n","\n","## Challenges:\n","- Privacy concerns.\n","- Bias and fairness.\n","- Environmental factors affecting accuracy.\n","\n","## Future:\n","Ongoing research in emotion recognition, mask detection, and improved algorithms will shape the future. Balancing convenience, security, and privacy remains a key challenge."]},{"cell_type":"markdown","metadata":{},"source":["## **Chapter 2: Approaches We Will Use**\n","\n","### There are  so many approaches we can use to tackle this problem. Main ones being;\n","\n","- **LBPH (Local Binary Pattern Histogram)**:\n","    - LBPH is a texture-based approach that works well for face recognition.\n","    - It analyzes the texture of facial features.\n","    - Implemented using libraries like OpenCV.\n","\n","- **Eigenfaces**:\n","    - Eigenfaces is a PCA-based approach.\n","    - It represents faces as linear combinations of eigenfaces.\n","    - Python libraries like scikit-learn can be used for implementation.\n","\n","- **Fisherfaces**:\n","    - Fisherfaces is an LDA-based approach.\n","    - It maximizes the ratio of between-class scatter to within-class scatter.\n","    - Implementation can be done using libraries like OpenCV.\n","\n","- **Deep Learning (Convolutional Neural Networks)**:\n","    - Deep learning methods, especially CNNs, have achieved state-of-the-art results.\n","    - Libraries like TensorFlow and PyTorch provide powerful tools for training deep learning models.\n","\n","- **Local Feature-Based (SIFT, SURF)**:\n","    - Local feature-based methods extract distinctive keypoints and descriptors.\n","    - Algorithms like SIFT and SURF are commonly used for this purpose.\n","    - OpenCV provides support for these methods.\n","\n","- **HOG (Histogram of Oriented Gradients)**:\n","    - HOG is a feature descriptor that captures the shape and appearance of faces.\n","    - It's effective for face detection and can be combined with classifiers.\n","    - OpenCV's HOGDescriptor is commonly used.\n","\n","- **Face Recognition Library**:\n","    - The face_recognition library simplifies face recognition using a pre-trained model.\n","    - It's built on top of dlib and is user-friendly for Python developers.\n","\n","- **Facial Landmark Detection**:\n","    - Detecting facial landmarks is a crucial step in face recognition.\n","    - Libraries like dlib and OpenCV provide tools for facial landmark detection.\n","    - It's often combined with other methods.\n","\n","- **Transfer Learning (Pre-trained Models)**:\n","    - Transfer learning involves using pre-trained models for face recognition tasks.\n","    - Models like VGGFace, FaceNet, and OpenFace can be fine-tuned for specific applications.\n","\n","- **3D Face Recognition**:\n","    - 3D face recognition methods use depth information for improved accuracy.\n","    - Depth sensors or 3D cameras are used to capture facial geometry.\n","    - Python libraries like Open3D and depthAI can be utilized."]},{"cell_type":"markdown","metadata":{},"source":["### In our project, we've chosen three distinct facial recognition approaches to leverage their specific strengths:\n","\n","1. **LBPH (Local Binary Pattern Histogram):** LBPH is ideal for texture-based facial analysis, handling variations in lighting and expressions effectively.\n","\n","2. **face_recognition Library:** This user-friendly library simplifies pre-trained model usage, aligning with our goal of a quick and friendly user experience.\n","\n","3. **Convolutional Neural Networks (CNNs):** CNNs offer exceptional performance in complex visual tasks, making them perfect for high-accuracy face recognition. We can fine-tune pre-trained CNN models using libraries like TensorFlow or PyTorch.\n","\n","These three approaches provide a comprehensive solution tailored to our project's needs, combining texture-based analysis, user-friendliness, and deep learning capabilities."]},{"cell_type":"markdown","metadata":{},"source":["## **Chapter 3: Implementing Our Approaches**"]},{"cell_type":"markdown","metadata":{},"source":["### **3.1: Implement the necesarry libraries**"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import cv2\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","import pickle\n","import face_recognition"]},{"cell_type":"markdown","metadata":{},"source":["### **3.2: LPHB Approach**"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","LBPH (Local Binary Pattern Histogram) is a texture-based face recognition technique that operates on grayscale images to identify individuals based on their facial features. It achieves face recognition results through the following steps:\n","\n","#### 1. Image Preprocessing\n","   - The input facial images are converted to grayscale to simplify processing.\n","   - Grayscale images are less sensitive to variations in color and lighting.\n","\n","#### 2. Local Binary Pattern (LBP) Computation\n","   - LBP is a texture analysis operator that focuses on the relationships between pixel values in an image.\n","   - For each pixel in the grayscale image, LBP encodes its relationship with its neighboring pixels.\n","   - A binary code is generated for each pixel, indicating whether neighboring pixels have higher or lower intensity values compared to the central pixel.\n","\n","#### 3. Histogram Generation\n","   - LBP histograms are created for each facial image in the training dataset.\n","   - The histograms represent the distribution of LBP patterns within each face.\n","   - These histograms capture the unique texture patterns of individual faces.\n","\n","#### 4. Training Phase\n","   - During the training phase, LBPH constructs LBP histograms for known faces in the dataset.\n","   - These histograms serve as reference representations for each known face.\n","   - The model stores these reference histograms along with corresponding labels (identifiers for each individual).\n","\n","#### 5. Recognition Phase\n","   - To recognize a face, LBPH computes the LBP histogram of the test face.\n","   - It then compares the test histogram with the reference histograms of known faces.\n","   - Similarity measures (e.g., Chi-Square or Euclidean distance) are used to find the closest match between the test histogram and the reference histograms.\n","   - The known face label associated with the closest match is considered the recognized identity.\n","\n","#### 6. Thresholding and Confidence\n","   - LBPH may apply a confidence threshold to the recognition result.\n","   - If the confidence (similarity measure) between the test face and the closest reference face is below the threshold, the identity is marked as \"Unknown.\"\n","   - Thresholding helps control false positives and enhances recognition accuracy.\n","\n","LBPH face recognition is known for its robustness to changes in lighting conditions and facial expressions. It is particularly suitable for smaller to medium-sized face recognition applications where computational efficiency is essential.\n","\n","Remember that LBPH may have limitations in handling extreme pose variations and requires a sufficiently diverse training dataset to achieve accurate recognition results.\n"]},{"cell_type":"markdown","metadata":{},"source":["#### **3.2.2: Implementation:**"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Function to create and train LBPH model\n","def create_and_train_lbph_model(images_dir, model_file_path, label_map_file):\n","    # Initialize LBPH model\n","    lbph_model = cv2.face.LBPHFaceRecognizer_create()\n","    faces = []\n","    labels = []\n","\n","    # List image files in the directory\n","    image_files = sorted(os.listdir(images_dir))\n","\n","    # Process each image\n","    for image_name in tqdm(image_files, desc='Processing images'):\n","        image_path = os.path.join(images_dir, image_name)\n","        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","        # Check if the image was loaded successfully\n","        if image is not None:\n","            faces.append(image)\n","            label = os.path.splitext(image_name)[0]\n","            labels.append(label)\n","\n","    # Convert labels to integers\n","    unique_labels = np.unique(labels)\n","    label_to_int = {label: i for i, label in enumerate(unique_labels)}\n","    int_labels = [label_to_int[label] for label in labels]\n","\n","    # Train the LBPH model\n","    lbph_model.train(faces, np.array(int_labels))\n","\n","    # Save the model to a file\n","    lbph_model.write(model_file_path)\n","\n","    # Save the label-to-integer mapping for later use\n","    with open(label_map_file, 'wb') as f:\n","        pickle.dump(label_to_int, f)\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["\n","def lbph_face_recognition(image, model, label_map):\n","    # Convert image to grayscale\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","    # Detect faces in the image\n","    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","\n","    # Check if any faces were found\n","    if len(faces) == 0:\n","        print(\"No faces found in the image for LBPH method.\")\n","\n","    # Process each detected face\n","    for (x, y, w, h) in faces:\n","        # Extract the face region\n","        face_img = gray[y:y+h, x:x+w]\n","\n","        # Recognize the face using the LBPH model\n","        label, confidence = model.predict(face_img)\n","        name = label_map.get(label, \"Unknown\") if confidence < 100 else \"Unknown\"\n","\n","        # Draw a rectangle around the face and label it\n","        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n","        cv2.putText(image, name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 2)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def display_lbph_results(image_path, model, label_map):\n","    \"\"\"\n","    Applies LBPH face recognition to the image and displays the result.\n","    \"\"\"\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        print(f\"Failed to load image from {image_path}\")\n","        return\n","    \n","    # Apply LBPH face recognition\n","    lbph_face_recognition(image, model, label_map)\n","    \n","    # Display the result\n","    cv2.imshow('LBPH Result', image)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["lbph_model = cv2.face.LBPHFaceRecognizer_create()\n","lbph_model.read('C:/Users/User/Downloads/archive/img_align_celeba/lbph_model.xml')\n","with open(\"C:/Users/User/Downloads/archive/img_align_celeba/label_map.pkl\", 'rb') as f:\n","    int_to_label = pickle.load(f)\n","\n","# Display LBPH results\n","display_lbph_results(\"C:/Users/User/Pictures/test.jpg\", lbph_model, int_to_label)\n"]},{"cell_type":"markdown","metadata":{},"source":["### **3.3 Face Recognition Library:**"]},{"cell_type":"markdown","metadata":{},"source":["### **3.3.1 Face Recognition Library Approach:**\n","The face_recognition library is a Python module that leverages deep learning and computer vision techniques to achieve accurate face recognition. It provides a high-level interface for recognizing and identifying faces in images. Here's how this approach works:\n","\n","#### 1. Facial Feature Extraction\n","   - The library employs a deep learning model, typically a convolutional neural network (CNN), to extract facial features from the input image.\n","   - These features capture unique facial patterns, including key landmarks and spatial information.\n","\n","#### 2. Pretrained Models\n","   - Face recognition using the face_recognition library relies on pretrained deep learning models.\n","   - These models have been trained on a large dataset of faces, enabling them to generalize well to various facial appearances.\n","\n","#### 3. Face Encoding\n","   - Once the facial features are extracted, the library encodes the face into a numerical representation known as a face encoding.\n","   - Face encodings are high-dimensional vectors that represent the characteristics of a face.\n","   - Each face encoding is unique to a specific individual.\n","\n","#### 4. Comparison and Matching\n","   - To recognize a face, the library compares the computed face encoding of the test face with a database of known face encodings.\n","   - It calculates the similarity between the test face encoding and each known face encoding.\n","   - Similarity measures like Euclidean distance or cosine similarity are often used for comparison.\n","   - The library identifies the known face with the closest matching face encoding.\n","\n","#### 5. Thresholding and Confidence\n","   - A confidence threshold may be applied to determine whether the recognition result is sufficiently confident.\n","   - If the similarity score between the test face and the closest known face falls below the threshold, the identity is considered \"Unknown.\"\n","   - Thresholding helps control false positives and ensures reliable recognition.\n","\n","#### 6. Real-Time Recognition\n","   - One of the advantages of the face_recognition library is its real-time face recognition capabilities.\n","   - It can process video streams or camera feeds to recognize faces in real-time applications.\n","\n","This approach is powerful and accurate due to the use of deep learning models for feature extraction. It excels in recognizing faces across various lighting conditions, poses, and facial expressions. However, it may require substantial computational resources and may not be as lightweight as some other methods.\n","\n","In summary, the face_recognition library simplifies the process of incorporating state-of-the-art face recognition into Python applications, making it a popular choice for face recognition tasks."]},{"cell_type":"markdown","metadata":{},"source":["#### **3.3.2: Implementation**"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def get_known_face_encodings(images_dir):\n","    known_face_encodings = []\n","    known_face_names = []\n","\n","    for image_name in os.listdir(images_dir):\n","        image_path = os.path.join(images_dir, image_name)\n","        current_image = face_recognition.load_image_file(image_path)\n","        face_encodings = face_recognition.face_encodings(current_image)\n","        if face_encodings:\n","            current_face_encoding = face_encodings[0]\n","            label = os.path.splitext(image_name)[0]\n","            known_face_encodings.append(current_face_encoding)\n","            known_face_names.append(label)\n","        else:\n","            print(f\"No faces found in the image: {image_name}\")\n","\n","    return known_face_encodings, known_face_names\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["\n","def fr_face_recognition(image, known_face_encodings, known_face_names):\n","    rgb_image = image[:, :, ::-1]\n","    face_locations = face_recognition.face_locations(rgb_image)\n","    if len(face_locations) == 0:\n","        print(\"No faces found in the image for face_recognition method.\")\n","\n","    face_encodings = face_recognition.face_encodings(rgb_image, face_locations)\n","    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n","        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n","        name = \"Unknown\"\n","        if len(matches) > 0:\n","            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n","            best_match_index = np.argmin(face_distances)\n","            if matches[best_match_index]:\n","                name = known_face_names[best_match_index]\n","        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n","        cv2.putText(image, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 1)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def display_fr_results(image_path, known_face_encodings, known_face_names):\n","    \"\"\"\n","    Applies face recognition using the face_recognition library to the image and displays the result.\n","    \"\"\"\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        print(f\"Failed to load image from {image_path}\")\n","        return\n","    \n","    # Apply face recognition using face_recognition library\n","    fr_face_recognition(image, known_face_encodings, known_face_names)\n","    \n","    # Display the result\n","    cv2.imshow('Face Recognition Result', image)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m known_face_encodings, known_face_names \u001b[38;5;241m=\u001b[39m \u001b[43mget_known_face_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/User/Downloads/archive/img_align_celeba/tmp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Display face_recognition results\u001b[39;00m\n\u001b[0;32m      4\u001b[0m display_fr_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/User/Pictures/test.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, known_face_encodings, known_face_names)\n","Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mget_known_face_encodings\u001b[1;34m(images_dir)\u001b[0m\n\u001b[0;32m      6\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(images_dir, image_name)\n\u001b[0;32m      7\u001b[0m current_image \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mload_image_file(image_path)\n\u001b[1;32m----> 8\u001b[0m face_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m face_encodings:\n\u001b[0;32m     10\u001b[0m     current_face_encoding \u001b[38;5;241m=\u001b[39m face_encodings[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\face_recognition\\api.py:213\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mface_encodings\u001b[39m(face_image, known_face_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_jitters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    Given an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    :return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     raw_landmarks \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_face_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_face_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(face_encoder\u001b[38;5;241m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n","File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\face_recognition\\api.py:156\u001b[0m, in \u001b[0;36m_raw_face_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raw_face_landmarks\u001b[39m(face_image, face_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m face_locations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m         face_locations \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_face_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         face_locations \u001b[38;5;241m=\u001b[39m [_css_to_rect(face_location) \u001b[38;5;28;01mfor\u001b[39;00m face_location \u001b[38;5;129;01min\u001b[39;00m face_locations]\n","File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\face_recognition\\api.py:105\u001b[0m, in \u001b[0;36m_raw_face_locations\u001b[1;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mface_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["known_face_encodings, known_face_names = get_known_face_encodings(\"C:/Users/User/Downloads/archive/img_align_celeba/tmp\")\n","\n","# Display face_recognition results\n","display_fr_results(\"C:/Users/User/Pictures/test.jpg\", known_face_encodings, known_face_names)"]},{"cell_type":"markdown","metadata":{},"source":["# -------------------------"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification report:\n","              precision    recall  f1-score   support\n","\n","           6       0.00      0.00      0.00       1.0\n","           9       0.00      0.00      0.00       1.0\n","          10       0.00      0.00      0.00       1.0\n","          15       0.00      0.00      0.00       1.0\n","          16       0.00      0.00      0.00       1.0\n","          18       0.00      0.00      0.00       1.0\n","          19       0.00      0.00      0.00       1.0\n","          24       0.00      0.00      0.00       1.0\n","          25       0.00      0.00      0.00       1.0\n","          26       0.00      0.00      0.00       0.0\n","          27       0.00      0.00      0.00       0.0\n","          28       0.00      0.00      0.00       0.0\n","          30       0.00      0.00      0.00       1.0\n","          31       0.00      0.00      0.00       0.0\n","          33       0.00      0.00      0.00       1.0\n","          38       0.00      0.00      0.00       1.0\n","          42       0.00      0.00      0.00       0.0\n","          43       0.00      0.00      0.00       0.0\n","          45       0.00      0.00      0.00       1.0\n","          54       0.00      0.00      0.00       0.0\n","          55       0.00      0.00      0.00       1.0\n","          58       0.00      0.00      0.00       0.0\n","          60       0.00      0.00      0.00       1.0\n","          66       0.00      0.00      0.00       0.0\n","          67       0.00      0.00      0.00       1.0\n","          68       0.00      0.00      0.00       1.0\n","          69       0.00      0.00      0.00       1.0\n","          76       0.00      0.00      0.00       0.0\n","          83       0.00      0.00      0.00       0.0\n","          92       0.00      0.00      0.00       0.0\n","          96       0.00      0.00      0.00       1.0\n","          97       0.00      0.00      0.00       1.0\n","          98       0.00      0.00      0.00       0.0\n","         103       0.00      0.00      0.00       0.0\n","         104       0.00      0.00      0.00       1.0\n","         108       0.00      0.00      0.00       1.0\n","         111       0.00      0.00      0.00       1.0\n","         112       0.00      0.00      0.00       1.0\n","         114       0.00      0.00      0.00       1.0\n","         115       0.00      0.00      0.00       0.0\n","         118       0.00      0.00      0.00       1.0\n","         120       0.00      0.00      0.00       1.0\n","         124       0.00      0.00      0.00       0.0\n","         125       0.00      0.00      0.00       0.0\n","         134       0.00      0.00      0.00       0.0\n","         135       0.00      0.00      0.00       0.0\n","         137       0.00      0.00      0.00       0.0\n","         141       0.00      0.00      0.00       1.0\n","         142       0.00      0.00      0.00       1.0\n","         143       0.00      0.00      0.00       0.0\n","         144       0.00      0.00      0.00       0.0\n","         146       0.00      0.00      0.00       1.0\n","         150       0.00      0.00      0.00       0.0\n","         153       0.00      0.00      0.00       0.0\n","         159       0.00      0.00      0.00       1.0\n","         160       0.00      0.00      0.00       0.0\n","         166       0.00      0.00      0.00       0.0\n","         170       0.00      0.00      0.00       0.0\n","         171       0.00      0.00      0.00       0.0\n","         172       0.00      0.00      0.00       0.0\n","         173       0.00      0.00      0.00       1.0\n","         176       0.00      0.00      0.00       1.0\n","         177       0.00      0.00      0.00       1.0\n","         180       0.00      0.00      0.00       1.0\n","         181       0.00      0.00      0.00       1.0\n","         184       0.00      0.00      0.00       1.0\n","         185       0.00      0.00      0.00       1.0\n","         192       0.00      0.00      0.00       0.0\n","         194       0.00      0.00      0.00       1.0\n","         196       0.00      0.00      0.00       1.0\n","         197       0.00      0.00      0.00       1.0\n","         199       0.00      0.00      0.00       0.0\n","         200       0.00      0.00      0.00       1.0\n","         201       0.00      0.00      0.00       1.0\n","         203       0.00      0.00      0.00       0.0\n","         207       0.00      0.00      0.00       0.0\n","         216       0.00      0.00      0.00       1.0\n","         218       0.00      0.00      0.00       0.0\n","         219       0.00      0.00      0.00       1.0\n","         222       0.00      0.00      0.00       1.0\n","         224       0.00      0.00      0.00       1.0\n","         225       0.00      0.00      0.00       0.0\n","         234       0.00      0.00      0.00       0.0\n","         236       0.00      0.00      0.00       1.0\n","         240       0.00      0.00      0.00       1.0\n","         247       0.00      0.00      0.00       1.0\n","\n","    accuracy                           0.00      50.0\n","   macro avg       0.00      0.00      0.00      50.0\n","weighted avg       0.00      0.00      0.00      50.0\n","\n","Accuracy: 0.0\n","Predicted label: 022009.jpg\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["import cv2\n","import os\n","import numpy as np\n","from sklearn import svm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","def load_dataset(dataset_path):\n","    images = []\n","    labels = []\n","    label_map = {}\n","\n","    if not os.path.exists(dataset_path):\n","        print(f\"Dataset path {dataset_path} does not exist.\")\n","        return np.array(images), np.array(labels), label_map\n","\n","    for file in os.listdir(dataset_path):\n","        img_path = os.path.join(dataset_path, file)\n","        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n","        if img is not None:\n","            # Extract label from file name here\n","            # For example, if file name is 'label_01.jpg'\n","            label = file.split('_')[0]  # Adjust this based on your file naming convention\n","\n","            if label not in label_map:\n","                label_map[label] = len(label_map)\n","\n","            img = cv2.resize(img, (64, 64)).flatten()\n","            images.append(img)\n","            labels.append(label_map[label])\n","        else:\n","            print(f\"Failed to load image: {img_path}\")\n","\n","    return np.array(images), np.array(labels), label_map\n","\n","\n","\n","def train_svm_classifier(X, y):\n","    # Ensure there is data to train on\n","    if X.size == 0:\n","        print(\"No data available for training.\")\n","        return None\n","\n","    # Split the dataset into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Initialize and train the SVM classifier\n","    classifier = svm.SVC(kernel='linear', probability=True)\n","    classifier.fit(X_train, y_train)\n","\n","    # Test the classifier on the test set\n","    y_pred = classifier.predict(X_test)\n","    print(f\"Classification report:\\n{classification_report(y_test, y_pred)}\")\n","    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n","\n","    return classifier\n","\n","def predict(classifier, image, label_map):\n","    if classifier is None:\n","        print(\"Classifier is not trained.\")\n","        return \"Unknown\"\n","\n","    processed_img = cv2.resize(image, (64, 64)).flatten()\n","    label = classifier.predict([processed_img])[0]\n","    return list(label_map.keys())[list(label_map.values()).index(label)]\n","\n","# Path to your dataset\n","dataset_path = 'C:/Users/User/Downloads/archive/img_align_celeba/tmp'\n","\n","# Load the dataset\n","X, y, label_map = load_dataset(dataset_path)\n","\n","# Train the SVM classifier\n","classifier = train_svm_classifier(X, y)\n","\n","# Example: predict a new image\n","test_image_path = 'C:/Users/User/Pictures/test.jpg'\n","if os.path.exists(test_image_path):\n","    test_image = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE)\n","    predicted_label = predict(classifier, test_image, label_map)\n","    print(f\"Predicted label: {predicted_label}\")\n","else:\n","    print(f\"Test image {test_image_path} not found.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## **3.4: CNN**"]},{"cell_type":"markdown","metadata":{},"source":["### **3.4.1 CNN APROACH:**\n","\n","### 1. **Data Preparation**\n","- **Dataset**: A large dataset of labeled face images is required. Each label typically represents a person, and there should be multiple images per person.\n","- **Preprocessing**: Images are preprocessed to have a consistent size and format. This usually includes resizing, normalization (scaling pixel values), and sometimes augmentation (to increase data variability).\n","\n","### 2. **Designing the CNN**\n","- **Architecture**: A CNN consists of convolutional layers, pooling layers, and fully connected layers. Convolutional layers extract features from images, pooling layers reduce the spatial dimensions, and fully connected layers make predictions.\n","- **Input Layer**: The network accepts an input image (of a fixed size, e.g., 224x224 pixels).\n","- **Convolutional Layers**: These layers apply filters to the image to learn features (like edges, textures). Each filter produces a feature map.\n","- **Pooling Layers**: Commonly following convolutional layers, these reduce the spatial dimensions of the feature maps, helping in reducing computation and controlling overfitting.\n","- **Fully Connected Layers**: Towards the end of the network, these layers interpret the features and output a prediction. In face recognition, the final layer typically has as many neurons as the number of people (classes) in the dataset.\n","\n","### 3. **Training the CNN**\n","- **Loss Function**: A loss function measures how well the model performs. For classification, cross-entropy loss is common.\n","- **Optimizer**: An algorithm like Adam or SGD is used to minimize the loss function by adjusting the weights of the network.\n","- **Backpropagation**: The process of updating the weights of the network using gradients calculated from the loss function.\n","- **Epochs**: Training involves passing the entire dataset through the CNN multiple times.\n","\n","### 4. **Using the CNN for Face Recognition**\n","- **Face Detection**: Before feeding an image to the CNN, a face needs to be detected and cropped from the image.\n","- **Feature Extraction**: The trained CNN extracts features from the detected face.\n","- **Classification**: The network classifies the face based on the learned features and provides a label (e.g., a person's name).\n","\n","### 5. **Evaluation**\n","- **Testing**: The model is tested on new images that weren't used during training to evaluate its performance.\n","- **Metrics**: Accuracy, precision, recall, and F1-score are common metrics to assess the model.\n","\n","### Practical Considerations\n","- **Dataset Size and Quality**: A large and diverse dataset is crucial for good performance.\n","- **Overfitting**: A common issue where the model performs well on training data but poorly on new data. Techniques like dropout, data augmentation, or using a simpler model can help.\n","- **Computational Resources**: Training a CNN requires significant computational power, typically provided by GPUs.\n","\n","### Pre-trained Models\n","Instead of training a CNN from scratch, it's common to use pre-trained models (like VGGFace, FaceNet) and fine-tune them on a specific dataset, which saves time and resources.\n","\n","### Conclusion\n","The CNN approach to face recognition is powerful but complex, requiring careful design, a large dataset, and significant computational resources. Its effectiveness lies in the ability of CNNs to learn and generalize from facial features, making them suitable for"]},{"cell_type":"markdown","metadata":{},"source":["###"]},{"cell_type":"markdown","metadata":{},"source":["### **3.4.2:Implementation** "]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOIjOFiUMyttezQ9wGD6JzC","collapsed_sections":[],"name":"40. Facial Recognition.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
